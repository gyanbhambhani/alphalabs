"""
Quantitative Analysis Tools for AI Stock Terminal.

Each tool:
1. Fetches/computes data
2. Returns a ChartSpec or TableSpec
3. Includes a pre-written explanation (no AI needed)

Tools:
- find_similar_periods: ChromaDB semantic search (KILLER FEATURE)
- calculate_volatility_regimes: K-means volatility clustering
- calculate_risk_metrics: VaR, CVaR, max drawdown
- calculate_returns_distribution: Forward returns histogram
- calculate_sharpe_evolution: Rolling Sharpe ratio
- calculate_correlation_matrix: Multi-stock correlation heatmap
"""

import asyncio
import logging
from datetime import datetime, timedelta
from typing import Optional

import numpy as np
import pandas as pd
import yfinance as yf

from core.ai.models import ChartSpec, TableSpec, ToolResult, ToolExecutionError

logger = logging.getLogger(__name__)


# =============================================================================
# Data Fetching
# =============================================================================

async def fetch_stock_data(
    symbol: str,
    days: int = 400,
    max_age_minutes: int = 15
) -> pd.DataFrame:
    """
    Fetch stock data from yfinance.
    
    Runs in thread pool to avoid blocking async loop.
    """
    def _fetch():
        ticker = yf.Ticker(symbol)
        df = ticker.history(period=f"{days}d")
        
        if df.empty:
            raise ValueError(f"No data returned for {symbol}")
        
        # Normalize column names
        df.columns = [c.lower() for c in df.columns]
        
        return df
    
    return await asyncio.to_thread(_fetch)


def downsample_for_viz(data: list, max_points: int = 200) -> list:
    """Downsample data for visualization while preserving shape."""
    if len(data) <= max_points:
        return data
    
    # Simple downsampling - take evenly spaced points
    indices = np.linspace(0, len(data) - 1, max_points).astype(int)
    return [data[i] for i in indices]


# =============================================================================
# KILLER FEATURE: Similar Periods using ChromaDB
# =============================================================================

async def find_similar_periods(
    symbol: str,
    n_results: int = 10,
    lookback_days: int = 400
) -> ToolResult:
    """
    Find historically similar periods using ChromaDB embeddings.
    
    THE KILLER FEATURE - answers "what happened historically?" with data.
    
    Uses existing embeddings generated by generate_sp500_embeddings.py.
    """
    try:
        from core.semantic.encoder import MarketStateEncoder
        from core.semantic.vector_db import VectorDatabase
        
        # 1. Fetch current data
        df = await fetch_stock_data(symbol, days=lookback_days)
        
        if len(df) < 252:
            raise ValueError(f"Insufficient data for {symbol}: {len(df)} rows")
        
        # 2. Encode current market state
        encoder = MarketStateEncoder()
        
        current_state = encoder.encode(
            date=str(df.index[-1].date()),
            close=df['close'],
            high=df.get('high'),
            low=df.get('low'),
            volume=df.get('volume')
        )
        
        # 3. Search ChromaDB for similar historical periods
        vector_db = VectorDatabase(
            persist_directory="./chroma_data",
            symbol=symbol.upper()
        )
        
        count = vector_db.get_count()
        if count == 0:
            raise ValueError(f"No embeddings found for {symbol}")
        
        similar = vector_db.search(
            query_vector=current_state.vector,
            top_k=n_results + 30  # Extra to filter out recent dates
        )
        
        # 4. Filter out recent dates (need forward returns to be meaningful)
        cutoff = (datetime.now() - timedelta(days=21)).strftime('%Y-%m-%d')
        periods = []
        
        for match in similar:
            if match.date >= cutoff:
                continue
            
            forward_1m = match.metadata.get('forward_1m_return')
            forward_3m = match.metadata.get('forward_3m_return')
            
            periods.append({
                'date': match.date,
                'similarity': round(match.similarity, 3),
                'forward_1m': forward_1m,
                'forward_3m': forward_3m,
                'outcome': _describe_outcome(forward_1m),
                'price': match.metadata.get('price', 0),
                'volatility': match.metadata.get('volatility_21d', 0),
            })
            
            if len(periods) >= n_results:
                break
        
        if not periods:
            raise ValueError("No historical periods found")
        
        # 5. Calculate statistics
        forward_returns = [
            p['forward_1m'] for p in periods 
            if p['forward_1m'] is not None
        ]
        
        if forward_returns:
            avg_return = float(np.mean(forward_returns))
            positive_count = sum(1 for r in forward_returns if r > 0)
            positive_rate = positive_count / len(forward_returns)
        else:
            avg_return = 0.0
            positive_count = 0
            positive_rate = 0.5
        
        # 6. Create histogram data for distribution chart
        histogram = _create_histogram(forward_returns)
        
        # 7. Generate chart spec
        chart = ChartSpec(
            type='similar_periods',
            data={
                'periods': periods,
                'current_date': str(df.index[-1].date()),
                'current_price': float(df['close'].iloc[-1]),
                'current_volatility': float(current_state.metadata.get('volatility_21d', 0)),
                'avg_forward_1m': avg_return,
                'positive_rate': positive_rate,
                'positive_count': positive_count,
                'total_count': len(periods),
                'histogram': histogram
            },
            config={
                'title': f'Historical Periods Similar to Today ({symbol})',
                'symbol': symbol
            }
        )
        
        # 8. Pre-written explanation (no AI needed)
        if positive_rate > 0.7:
            sentiment = "Historically BULLISH pattern"
            action = "History suggests favorable odds."
        elif positive_rate < 0.3:
            sentiment = "Historically BEARISH pattern"
            action = "History suggests caution."
        else:
            sentiment = "Mixed historical outcomes"
            action = "No clear directional bias from history."
        
        explanation = (
            f"Found {len(periods)} periods similar to current conditions. "
            f"Stock went UP {positive_count}/{len(periods)} times (1 month later). "
            f"Average outcome: {avg_return:+.1%}. {sentiment}. {action}"
        )
        
        return ToolResult(
            name='similar_periods',
            chart=chart,
            explanation=explanation,
            metrics={
                'avg_forward_1m': avg_return,
                'positive_rate': positive_rate,
                'n_periods': len(periods)
            }
        )
        
    except ImportError as e:
        raise ToolExecutionError('similar_periods', f"Missing dependency: {e}")
    except Exception as e:
        logger.error(f"similar_periods failed: {e}", exc_info=True)
        raise ToolExecutionError('similar_periods', str(e))


def _describe_outcome(forward_1m: Optional[float]) -> str:
    """Human-readable description of forward return outcome."""
    if forward_1m is None:
        return "Unknown"
    if forward_1m > 0.10:
        return "Strong rally (+10%+)"
    elif forward_1m > 0.05:
        return "Moderate gain (+5-10%)"
    elif forward_1m > 0.02:
        return "Slight gain (+2-5%)"
    elif forward_1m > -0.02:
        return "Flat (-2% to +2%)"
    elif forward_1m > -0.05:
        return "Slight loss (-2-5%)"
    elif forward_1m > -0.10:
        return "Moderate decline (-5-10%)"
    else:
        return "Sharp drop (-10%+)"


def _create_histogram(returns: list, bins: int = 10) -> dict:
    """Create histogram data for returns distribution."""
    if not returns:
        return {'bins': [], 'counts': [], 'labels': []}
    
    returns_array = np.array(returns)
    counts, bin_edges = np.histogram(returns_array, bins=bins)
    
    labels = [
        f"{bin_edges[i]*100:.0f}% to {bin_edges[i+1]*100:.0f}%"
        for i in range(len(counts))
    ]
    
    return {
        'bins': [(float(bin_edges[i]), float(bin_edges[i+1])) 
                 for i in range(len(counts))],
        'counts': counts.tolist(),
        'labels': labels
    }


# =============================================================================
# Volatility Regimes (K-means clustering)
# =============================================================================

async def calculate_volatility_regimes(
    symbol: str,
    lookback_days: int = 252
) -> ToolResult:
    """
    Detect volatility regime changes using K-means clustering.
    
    Identifies Low/Medium/High volatility periods.
    """
    try:
        df = await fetch_stock_data(symbol, days=lookback_days + 50)
        
        # Calculate rolling volatility
        df['returns'] = df['close'].pct_change()
        df['vol_20d'] = df['returns'].rolling(20).std() * np.sqrt(252)
        df = df.dropna()
        
        if len(df) < 50:
            raise ValueError(f"Insufficient data for volatility analysis")
        
        # K-means clustering for regime detection
        try:
            from sklearn.cluster import KMeans
            
            vol_data = df[['vol_20d']].values
            kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
            df['regime_num'] = kmeans.fit_predict(vol_data)
            
            # Map clusters to Low/Medium/High by center values
            centers = [(i, c[0]) for i, c in enumerate(kmeans.cluster_centers_)]
            centers_sorted = sorted(centers, key=lambda x: x[1])
            regime_map = {
                centers_sorted[0][0]: 'Low',
                centers_sorted[1][0]: 'Medium',
                centers_sorted[2][0]: 'High'
            }
            df['regime'] = df['regime_num'].map(regime_map)
            
        except ImportError:
            # Fallback to threshold-based if sklearn not available
            low_thresh = df['vol_20d'].quantile(0.33)
            high_thresh = df['vol_20d'].quantile(0.67)
            
            df['regime'] = pd.cut(
                df['vol_20d'],
                bins=[0, low_thresh, high_thresh, float('inf')],
                labels=['Low', 'Medium', 'High']
            )
        
        # Find regime change dates
        regime_changes = df[df['regime'] != df['regime'].shift()].index.tolist()
        regime_change_dates = [
            str(d.date()) if hasattr(d, 'date') else str(d) 
            for d in regime_changes
        ]
        
        # Current state
        current_regime = df['regime'].iloc[-1]
        current_vol = df['vol_20d'].iloc[-1]
        avg_vol = df['vol_20d'].mean()
        
        # Prepare chart data (downsampled)
        dates = [str(d.date()) if hasattr(d, 'date') else str(d) 
                 for d in df.index]
        
        chart = ChartSpec(
            type='volatility_regime',
            data={
                'dates': downsample_for_viz(dates),
                'prices': downsample_for_viz(df['close'].tolist()),
                'volatility': downsample_for_viz(df['vol_20d'].tolist()),
                'regimes': downsample_for_viz(df['regime'].tolist()),
                'regime_changes': regime_change_dates[-10:],  # Last 10 changes
                'current_regime': current_regime,
                'current_vol': float(current_vol),
                'avg_vol': float(avg_vol)
            },
            config={
                'title': f'{symbol} Volatility Regimes',
                'symbol': symbol
            }
        )
        
        # Pre-written explanation
        vol_description = "elevated" if current_vol > avg_vol else "below average"
        risk_note = (
            "Higher risk environment." if current_regime == 'High' 
            else "Relatively stable conditions." if current_regime == 'Low'
            else "Normal volatility conditions."
        )
        
        explanation = (
            f"{symbol} is currently in a {current_regime} volatility regime. "
            f"Current volatility is {current_vol:.1%} annualized ({vol_description}). "
            f"{risk_note}"
        )
        
        return ToolResult(
            name='volatility_regimes',
            chart=chart,
            explanation=explanation,
            metrics={
                'current_regime': current_regime,
                'current_vol': float(current_vol),
                'avg_vol': float(avg_vol)
            }
        )
        
    except Exception as e:
        logger.error(f"volatility_regimes failed: {e}", exc_info=True)
        raise ToolExecutionError('volatility_regimes', str(e))


# =============================================================================
# Risk Metrics (VaR, CVaR, Drawdown)
# =============================================================================

async def calculate_risk_metrics(
    symbol: str,
    lookback_days: int = 252,
    confidence: float = 0.95
) -> ToolResult:
    """
    Calculate risk metrics: VaR, CVaR, max drawdown.
    """
    try:
        df = await fetch_stock_data(symbol, days=lookback_days + 50)
        
        returns = df['close'].pct_change().dropna()
        
        if len(returns) < 50:
            raise ValueError("Insufficient data for risk metrics")
        
        # Historical VaR
        var_95 = float(returns.quantile(1 - confidence))
        
        # CVaR (Expected Shortfall)
        cvar_95 = float(returns[returns <= var_95].mean())
        
        # Max Drawdown
        cumulative = (1 + returns).cumprod()
        running_max = cumulative.cummax()
        drawdowns = (cumulative - running_max) / running_max
        max_drawdown = float(drawdowns.min())
        
        # Current drawdown
        current_drawdown = float(drawdowns.iloc[-1])
        
        # Volatility
        volatility = float(returns.std() * np.sqrt(252))
        
        # Create table
        table = TableSpec(
            title=f"Risk Metrics for {symbol}",
            columns=['Metric', 'Value', 'Interpretation'],
            rows=[
                {
                    'Metric': f'VaR ({confidence:.0%})',
                    'Value': f'{var_95:.2%}',
                    'Interpretation': f'{confidence:.0%} confidence daily losses won\'t exceed this'
                },
                {
                    'Metric': f'CVaR ({confidence:.0%})',
                    'Value': f'{cvar_95:.2%}',
                    'Interpretation': 'Expected loss when VaR is breached'
                },
                {
                    'Metric': 'Max Drawdown',
                    'Value': f'{max_drawdown:.2%}',
                    'Interpretation': 'Largest peak-to-trough decline'
                },
                {
                    'Metric': 'Current Drawdown',
                    'Value': f'{current_drawdown:.2%}',
                    'Interpretation': 'Current distance from peak'
                },
                {
                    'Metric': 'Annualized Vol',
                    'Value': f'{volatility:.1%}',
                    'Interpretation': 'Annualized return standard deviation'
                }
            ]
        )
        
        # Pre-written explanation
        risk_level = (
            "high" if volatility > 0.35 
            else "moderate" if volatility > 0.20 
            else "low"
        )
        
        drawdown_note = (
            f"Currently {abs(current_drawdown):.1%} below recent highs." 
            if current_drawdown < -0.05 
            else "Near recent highs."
        )
        
        explanation = (
            f"{symbol} has {risk_level} risk profile with {volatility:.1%} annualized volatility. "
            f"Worst-case daily loss (95% confidence): {abs(var_95):.1%}. "
            f"Historical max drawdown: {abs(max_drawdown):.1%}. {drawdown_note}"
        )
        
        return ToolResult(
            name='risk_metrics',
            table=table,
            explanation=explanation,
            metrics={
                'var_95': var_95,
                'cvar_95': cvar_95,
                'max_drawdown': max_drawdown,
                'volatility': volatility
            }
        )
        
    except Exception as e:
        logger.error(f"risk_metrics failed: {e}", exc_info=True)
        raise ToolExecutionError('risk_metrics', str(e))


# =============================================================================
# Returns Distribution
# =============================================================================

async def calculate_returns_distribution(
    symbol: str,
    lookback_days: int = 252
) -> ToolResult:
    """
    Calculate and visualize returns distribution.
    """
    try:
        df = await fetch_stock_data(symbol, days=lookback_days + 50)
        
        # Calculate various return windows
        returns_1d = df['close'].pct_change()
        returns_1w = df['close'].pct_change(5)
        returns_1m = df['close'].pct_change(21)
        
        # Statistics
        stats = {
            'daily': {
                'mean': float(returns_1d.mean()),
                'std': float(returns_1d.std()),
                'skew': float(returns_1d.skew()),
                'min': float(returns_1d.min()),
                'max': float(returns_1d.max())
            },
            'weekly': {
                'mean': float(returns_1w.mean()),
                'std': float(returns_1w.std()),
                'positive_rate': float((returns_1w > 0).mean())
            },
            'monthly': {
                'mean': float(returns_1m.mean()),
                'std': float(returns_1m.std()),
                'positive_rate': float((returns_1m > 0).mean())
            }
        }
        
        # Create histogram for visualization
        daily_hist = _create_histogram(returns_1d.dropna().tolist(), bins=20)
        
        chart = ChartSpec(
            type='returns_distribution',
            data={
                'histogram': daily_hist,
                'stats': stats,
                'recent_returns': {
                    '1d': float(returns_1d.iloc[-1]) if len(returns_1d) > 0 else 0,
                    '1w': float(returns_1w.iloc[-1]) if len(returns_1w) > 4 else 0,
                    '1m': float(returns_1m.iloc[-1]) if len(returns_1m) > 20 else 0,
                }
            },
            config={
                'title': f'{symbol} Returns Distribution',
                'symbol': symbol
            }
        )
        
        # Pre-written explanation
        skew = stats['daily']['skew']
        skew_desc = (
            "positively skewed (more upside)" if skew > 0.5
            else "negatively skewed (more downside)" if skew < -0.5
            else "roughly symmetric"
        )
        
        win_rate = stats['monthly']['positive_rate']
        win_desc = (
            f"positive {win_rate:.0%} of months"
        )
        
        explanation = (
            f"{symbol} returns are {skew_desc}. "
            f"Average monthly return: {stats['monthly']['mean']:.1%}, {win_desc}. "
            f"Recent performance: {stats['weekly']['mean']*52:.1%} annualized."
        )
        
        return ToolResult(
            name='returns_distribution',
            chart=chart,
            explanation=explanation,
            metrics=stats
        )
        
    except Exception as e:
        logger.error(f"returns_distribution failed: {e}", exc_info=True)
        raise ToolExecutionError('returns_distribution', str(e))


# =============================================================================
# Sharpe Evolution
# =============================================================================

async def calculate_sharpe_evolution(
    symbol: str,
    window: int = 252,
    lookback_days: int = 756
) -> ToolResult:
    """
    Calculate rolling Sharpe ratio over time.
    """
    try:
        df = await fetch_stock_data(symbol, days=lookback_days + 50)
        
        returns = df['close'].pct_change()
        
        # Rolling Sharpe (assuming 0% risk-free rate)
        rolling_mean = returns.rolling(window).mean() * 252
        rolling_std = returns.rolling(window).std() * np.sqrt(252)
        df['sharpe'] = rolling_mean / rolling_std.replace(0, np.nan)
        
        df = df.dropna(subset=['sharpe'])
        
        if len(df) < 50:
            raise ValueError("Insufficient data for Sharpe analysis")
        
        # Current and average Sharpe
        current_sharpe = float(df['sharpe'].iloc[-1])
        avg_sharpe = float(df['sharpe'].mean())
        max_sharpe = float(df['sharpe'].max())
        min_sharpe = float(df['sharpe'].min())
        
        # Prepare chart data
        dates = [str(d.date()) if hasattr(d, 'date') else str(d) 
                 for d in df.index]
        
        chart = ChartSpec(
            type='sharpe_evolution',
            data={
                'dates': downsample_for_viz(dates),
                'sharpe': downsample_for_viz(df['sharpe'].tolist()),
                'prices': downsample_for_viz(df['close'].tolist()),
                'current_sharpe': current_sharpe,
                'avg_sharpe': avg_sharpe,
                'thresholds': [0, 1, 2]  # Common Sharpe benchmarks
            },
            config={
                'title': f'{symbol} Rolling Sharpe Ratio ({window}d)',
                'symbol': symbol,
                'window': window
            }
        )
        
        # Pre-written explanation
        quality = (
            "excellent" if current_sharpe > 2
            else "good" if current_sharpe > 1
            else "average" if current_sharpe > 0
            else "poor"
        )
        
        trend = (
            "improving" if current_sharpe > avg_sharpe
            else "declining"
        )
        
        explanation = (
            f"{symbol} has a {quality} risk-adjusted return profile. "
            f"Current Sharpe: {current_sharpe:.2f} ({trend} vs {avg_sharpe:.2f} average). "
            f"Range: {min_sharpe:.2f} to {max_sharpe:.2f}."
        )
        
        return ToolResult(
            name='sharpe_evolution',
            chart=chart,
            explanation=explanation,
            metrics={
                'current_sharpe': current_sharpe,
                'avg_sharpe': avg_sharpe
            }
        )
        
    except Exception as e:
        logger.error(f"sharpe_evolution failed: {e}", exc_info=True)
        raise ToolExecutionError('sharpe_evolution', str(e))


# =============================================================================
# Correlation Matrix
# =============================================================================

async def calculate_correlation_matrix(
    symbols: list[str],
    lookback_days: int = 252
) -> ToolResult:
    """
    Calculate correlation matrix for multiple stocks.
    """
    try:
        if len(symbols) < 2:
            raise ValueError("Need at least 2 symbols for correlation")
        
        # Fetch data for all symbols
        dfs = {}
        for sym in symbols:
            try:
                df = await fetch_stock_data(sym, days=lookback_days + 50)
                dfs[sym] = df['close'].pct_change()
            except Exception as e:
                logger.warning(f"Could not fetch {sym}: {e}")
        
        if len(dfs) < 2:
            raise ValueError("Could not fetch enough symbols")
        
        # Combine and calculate correlation
        combined = pd.DataFrame(dfs)
        corr_matrix = combined.corr()
        
        # Prepare matrix data
        matrix_data = []
        for i, sym1 in enumerate(corr_matrix.columns):
            for j, sym2 in enumerate(corr_matrix.columns):
                matrix_data.append({
                    'x': sym2,
                    'y': sym1,
                    'value': float(corr_matrix.iloc[i, j])
                })
        
        chart = ChartSpec(
            type='correlation_heatmap',
            data={
                'symbols': list(corr_matrix.columns),
                'matrix': corr_matrix.values.tolist(),
                'matrix_flat': matrix_data
            },
            config={
                'title': 'Correlation Matrix',
                'color_scale': 'RdYlGn'
            }
        )
        
        # Find highest and lowest correlations (excluding diagonal)
        corr_pairs = []
        for i, sym1 in enumerate(corr_matrix.columns):
            for j, sym2 in enumerate(corr_matrix.columns):
                if i < j:
                    corr_pairs.append((sym1, sym2, corr_matrix.iloc[i, j]))
        
        corr_pairs.sort(key=lambda x: x[2], reverse=True)
        
        highest = corr_pairs[0] if corr_pairs else None
        lowest = corr_pairs[-1] if corr_pairs else None
        
        # Pre-written explanation
        if highest and lowest:
            explanation = (
                f"Analyzed correlations for {len(symbols)} stocks. "
                f"Highest: {highest[0]}-{highest[1]} ({highest[2]:.2f}). "
                f"Lowest: {lowest[0]}-{lowest[1]} ({lowest[2]:.2f}). "
                f"{'High correlations suggest limited diversification.' if highest[2] > 0.7 else ''}"
            )
        else:
            explanation = f"Correlation matrix for {len(symbols)} stocks."
        
        return ToolResult(
            name='correlation_matrix',
            chart=chart,
            explanation=explanation,
            metrics={
                'highest_corr': highest[2] if highest else 0,
                'lowest_corr': lowest[2] if lowest else 0
            }
        )
        
    except Exception as e:
        logger.error(f"correlation_matrix failed: {e}", exc_info=True)
        raise ToolExecutionError('correlation_matrix', str(e))


# =============================================================================
# Tool Registry
# =============================================================================

TOOL_FUNCTIONS = {
    'similar_periods': find_similar_periods,
    'volatility_regimes': calculate_volatility_regimes,
    'risk_metrics': calculate_risk_metrics,
    'returns_distribution': calculate_returns_distribution,
    'sharpe_evolution': calculate_sharpe_evolution,
    'correlation_matrix': calculate_correlation_matrix,
}


async def execute_tool(tool_name: str, symbol: str, **kwargs) -> ToolResult:
    """
    Execute a tool by name.
    
    Args:
        tool_name: Name of the tool to execute
        symbol: Stock symbol to analyze
        **kwargs: Additional tool-specific arguments
        
    Returns:
        ToolResult with chart/table and explanation
        
    Raises:
        ToolExecutionError if tool fails
    """
    func = TOOL_FUNCTIONS.get(tool_name)
    
    if not func:
        raise ToolExecutionError(tool_name, f"Unknown tool: {tool_name}")
    
    if tool_name == 'correlation_matrix':
        # Special case: needs list of symbols
        symbols = kwargs.get('symbols', [symbol])
        return await func(symbols)
    
    return await func(symbol, **kwargs)
